id_2017,id_2018,method,unique_id,random_state,title_2017,title_2018
SyOvg6jxx,By-7dz-AZ,naive,91c2b4d7-141e-4362-8d6d-303b3438b6c4,60,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,A Framework for the Quantitative Evaluation of Disentangled Representations
SyOvg6jxx,HJw8fAgA-,KNN,ced4530d-793e-4abf-9ad5-ef4fb2214a16,154,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,Learning Dynamic State Abstractions for Model-Based Reinforcement Learning
SyOvg6jxx,HkpRBFxRb,SPSM,afff834f-1976-4d4e-94f6-03c5d8dc3835,160,#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning
SkyQWDcex,S1v4N2l0-,naive,0bfc52ef-09e4-4a12-9b8d-b61557497847,60,A Context-aware Attention Network for Interactive Question Answering,Unsupervised Representation Learning by Predicting Image Rotations
SkyQWDcex,ryj0790hb,SPSM,71053412-1893-4511-a3e7-958bc80e697c,160,A Context-aware Attention Network for Interactive Question Answering,Incremental Learning through Deep Adaptation
SkyQWDcex,rk3pnae0b,KNN,dec14cce-e507-4884-be00-357aa06a02ac,122,A Context-aware Attention Network for Interactive Question Answering,Topic-Based Question Generation
BJO-BuT1g,rkWN3g-AZ,KNN,4bac0feb-fa06-4408-8664-0ef9c91b7074,114,A Learned Representation For Artistic Style,XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings
BJO-BuT1g,ryTp3f-0-,naive,1e18e268-aee9-484a-a760-b62f85ef6e61,60,A Learned Representation For Artistic Style,Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration
BJO-BuT1g,HkL7n1-0b,SPSM,cfaf5698-6632-4cbc-aac8-169850f53233,160,A Learned Representation For Artistic Style,Wasserstein Auto-Encoders
B1IzH7cxl,Hk0wHx-RW,SPSM,0765ca0f-e78c-4ec3-ae23-b77d34a1cd36,160,A Neural Stochastic Volatility Model,Learning Sparse Latent Representations with the Deep Copula Information Bottleneck
B1IzH7cxl,HyY0Ff-AZ,naive,e4891846-be64-4a78-8665-b113d0816935,60,A Neural Stochastic Volatility Model,Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients
B1IzH7cxl,HJJ23bW0b,KNN,7e1ffc12-0f3d-4f2f-9a12-6d289a533a9e,106,A Neural Stochastic Volatility Model,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks
SyK00v5xx,ByJ7obb0b,SPSM,3a0411ef-62a2-4d02-ace9-010a9cc07363,160,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,Understanding and Exploiting the Low-Rank Structure of Deep Networks
SyK00v5xx,H1I3M7Z0b,naive,cc104e6d-c2ab-44c5-8dab-67899428928c,60,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,WSNet: Learning Compact and Efficient Networks with Weight Sampling
SyK00v5xx,BJMuY-gRW,KNN,759b4f49-7d06-464a-9a1f-f2bf03aaefed,109,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs
r1BJLw9ex,ryup8-WCW,SPSM,549abedd-47e7-442d-9429-5903d603c85d,160,Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,Measuring the Intrinsic Dimension of Objective Landscapes
r1BJLw9ex,ry831QWAb,KNN,33050ddc-8e0a-40b3-b3f2-780e9edaa1be,103,Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK
r1BJLw9ex,S1HlA-ZAZ,naive,400588c4-c4d0-4c86-8b6e-5856b1023b91,60,Adjusting for Dropout Variance in Batch Normalization and Weight Initialization,The Kanerva Machine: A Generative Distributed Memory
rkuDV6iex,Sy4c-3xRW,naive,d25e76c1-53b2-422a-a32c-98d3008edd15,60,An Empirical Analysis of Deep Network Loss Surfaces,DropMax: Adaptive Stochastic Softmax
rkuDV6iex,SJIA6ZWC-,KNN,6992429a-017b-44ce-843a-0949365b752d,81,An Empirical Analysis of Deep Network Loss Surfaces,Stochastic Hyperparameter Optimization through Hypernetworks
rkuDV6iex,HypkN9yRW,SPSM,47db3661-b08d-439a-b740-76d29c960f27,160,An Empirical Analysis of Deep Network Loss Surfaces,DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer
Hk-mgcsgx,By-7dz-AZ,KNN,7af24f61-912d-4def-86e4-89441cc0cd86,91,An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,A Framework for the Quantitative Evaluation of Disentangled Representations
Hk-mgcsgx,SJa1Nk10b,SPSM,c53aaf49-f133-498f-b9a5-1384bcf5fcbe,160,An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy
Hk-mgcsgx,H1BLjgZCb,naive,7b575a9e-a6a5-4532-afc8-07e2729ef687,60,An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views,Generating Natural Adversarial Examples
SkYbF1slg,H1A5ztj3b,KNN,a06914cb-5ae3-442e-b287-a8525c43bbcd,126,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates
SkYbF1slg,SJky6Ry0W,SPSM,c58d4620-b1f3-4ed8-b173-d363aef380bc,160,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Learning Independent Causal Mechanisms
SkYbF1slg,S1WRibb0Z,naive,adfdf440-cf5a-4063-b373-726abbf0b45c,60,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax,Expressive power of recurrent neural networks
H1GEvHcee,SkZ-BnyCW,KNN,550925a7-0e97-425f-97e8-bbf4aaa0a556,135,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Learning Deep Generative Models With Discrete Latent Variables
H1GEvHcee,ry4S90l0b,naive,81d9ba63-433e-4b7e-b110-70d67d0d8a50,60,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,A Self-Training Method for Semi-Supervised GANs
H1GEvHcee,ByQZjx-0-,SPSM,3007960e-3a55-4d47-b7d9-6a79ffbadc74,160,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM,Faster Discovery of Neural Architectures by Searching for Paths in a Large Model
Hk1l9Xqxe,SySaJ0xCZ,SPSM,5fc25d18-aee2-454e-a658-bbd9ac5f15f9,160,BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Simple and efficient architecture search for Convolutional Neural Networks
Hk1l9Xqxe,ByW5yxgA-,KNN,ac425b5c-3943-4372-93f3-ca9b2b8c387e,128,BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Multiscale Hidden Markov Models For Covariance Prediction
Hk1l9Xqxe,ry80wMW0W,naive,a1319504-6d0c-423f-886b-f0b23763544a,60,BIOACOUSTIC SEGMENTATION BY HIERARCHICAL DIRICHLET PROCESS HIDDEN MARKOV MODEL,Hierarchical Subtask Discovery with Non-Negative Matrix Factorization
ryZqPN5xe,ryRh0bb0Z,SPSM,0ee1f74e-1b60-4dab-afab-b61c7ac21177,160,Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Multi-View Data Generation Without View Supervision
ryZqPN5xe,H1A5ztj3b,KNN,ee1e5652-2f96-4c8f-a3e4-75bafd684841,100,Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates
ryZqPN5xe,SJ-C6JbRW,naive,33648274-a908-4717-b8f2-3a4b718d7ef0,60,Beyond Fine Tuning: A Modular Approach to Learning on Small Data,Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent
SyxeqhP9ll,rkw-jlb0W,KNN,22f8c369-12c8-4c0e-abc7-e513ace88523,151,Calibrating Energy-based Generative Adversarial Networks,Deep Lipschitz networks and Dudley GANs
SyxeqhP9ll,rk1FQA0pW,naive,ddd15f15-7809-42ce-a7e5-073694b75f44,60,Calibrating Energy-based Generative Adversarial Networks,End-to-End Abnormality Detection in Medical Imaging
SyxeqhP9ll,ryjw_eAaZ,SPSM,1799fcbd-646a-4cf5-bd9f-56fb6805e903,160,Calibrating Energy-based Generative Adversarial Networks,Unsupervised Deep Structure Learning by Recursive Dependency Analysis
SJttqw5ge,S14EogZAZ,KNN,f4318390-db64-4fa2-810b-7a1c48c50df3,153,Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning
SJttqw5ge,BkVsWbbAW,SPSM,c1c6cc5c-e7d5-4512-9c25-e088818014b4,160,Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Deep Generative Dual Memory Network for Continual Learning
SJttqw5ge,rJNpifWAb,naive,4f78789c-a586-4913-949c-f56f4b7d0313,60,Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches
BkIqod5ll,H1Xw62kRZ,SPSM,a859ac48-9664-427a-a659-6a48d52b1451,160,Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis
BkIqod5ll,r1DPFCyA-,naive,f9f6bef1-3e0e-49a8-a307-6f109d9bed72,60,Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Discriminative k-shot learning using probabilistic models
BkIqod5ll,S1TgE7WR-,KNN,f4ca3e8f-c0aa-4eba-8400-9d372be44e62,134,Convolutional Neural Networks Generalization Utilizing the Data Graph Structure,Covariant Compositional Networks For Learning Graphs
H1VyHY9gg,HkwZSG-CZ,KNN,7e142094-6d87-45f9-85f7-0022b68237ce,65,Data Noising as Smoothing in Neural Network Language Models,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model
H1VyHY9gg,SkA-IE06W,SPSM,4524b026-4554-4728-9ac1-d49308a44ac8,160,Data Noising as Smoothing in Neural Network Language Models,When is a Convolutional Filter Easy to Learn?
H1VyHY9gg,rk4Fz2e0b,naive,6c08417e-2b92-47ba-b58b-401455767a4e,60,Data Noising as Smoothing in Neural Network Language Models,Graph Partition Neural Networks for Semi-Supervised Classification
HJ9rLLcxg,BJInEZsTb,naive,a3114756-998f-4104-a38b-2edd28e8f716,60,Dataset Augmentation in Feature Space,Learning Representations and Generative Models for 3D Point Clouds
HJ9rLLcxg,rJL6pz-CZ,KNN,dafda282-8d50-4a39-98bb-ed873773a31c,67,Dataset Augmentation in Feature Space,Transfer Learning on Manifolds via Learned Transport Operators
HJ9rLLcxg,rkaT3zWCZ,SPSM,00923ad9-2b8b-47aa-8b03-101fbf752b0f,160,Dataset Augmentation in Feature Space,Building Generalizable Agents with a Realistic and Rich 3D Environment
rkEFLFqee,rk8wKk-R-,KNN,007d52d8-f980-42a5-8843-5cf8174e83e5,68,Decomposing Motion and Content for Natural Video Sequence Prediction,Convolutional Sequence Modeling Revisited
rkEFLFqee,S14EogZAZ,naive,c15c9d94-ae97-466f-a333-ad032129b9cd,60,Decomposing Motion and Content for Natural Video Sequence Prediction,Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning
rkEFLFqee,BJluxbWC-,SPSM,e8d882c0-24da-4aa5-97f5-cc9d7542c14a,160,Decomposing Motion and Content for Natural Video Sequence Prediction,Unseen Class Discovery in Open-world Classification
HycUbvcge,rJWechg0Z,KNN,e54c3caa-b9c4-491d-9aef-66a0fbfc52cc,110,Deep Generalized Canonical Correlation Analysis,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation
HycUbvcge,SkYXvCR6W,naive,95750e17-e869-4601-b318-945db3a875a9,60,Deep Generalized Canonical Correlation Analysis,Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification
HycUbvcge,HJZiRkZC-,SPSM,6763c6c0-d977-4f8a-b58e-31e3ee2a3f71,160,Deep Generalized Canonical Correlation Analysis,Byte-Level Recursive Convolutional Auto-Encoder for Text
HyxQzBceg,HJ94fqApW,naive,075fdb48-21a3-4071-ae4f-9cd12745dbc9,60,Deep Variational Information Bottleneck,Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers
HyxQzBceg,rkfbLilAb,SPSM,9d3d362b-25bf-4992-97c1-6753d4013f56,160,Deep Variational Information Bottleneck,Improving Search Through A3C Reinforcement Learning Based Conversational Agent
HyxQzBceg,HkL7n1-0b,KNN,feccbda0-7fdf-4d9a-925c-a50c0cf892ce,157,Deep Variational Information Bottleneck,Wasserstein Auto-Encoders
SJJN38cge,H1bM1fZCW,KNN,d4d5e57c-7515-41c2-b8b0-4da02b9d28e4,111,Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks
SJJN38cge,rJ6iJmWCW,SPSM,a78e24d9-c783-45c8-8cfb-e9c511179e01,160,Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION
SJJN38cge,HyrCWeWCb,naive,11d3781c-352e-439d-8291-219111e91806,60,Distributed Transfer Learning for Deep Convolutional Neural Networks by Basic Probability Assignment,Trust-PCL: An Off-Policy Trust Region Method for Continuous Control
r10FA8Kxg,rytNfI1AZ,naive,a26f2884-5e19-44f4-8cd2-fc83b513ef5e,60,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Training wide residual networks for deployment using a single bit for each weight
r10FA8Kxg,HJMN-xWC-,SPSM,b105b6b8-7056-4c78-9bfb-5e5ce7d9f2a7,160,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Learning Parsimonious Deep Feed-forward Networks
r10FA8Kxg,ryj0790hb,KNN,523953e2-e799-46b8-a0da-75654c4184c0,147,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,Incremental Learning through Deep Adaptation
rkGabzZgl,rkHVZWZAZ,SPSM,b058b15b-3ffb-4ab9-a83c-107de0f0f304,160,Dropout with Expectation-linear Regularization,The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning
rkGabzZgl,S1NHaMW0b,KNN,ddffbc4f-be38-426e-8cda-ebc83144db3b,120,Dropout with Expectation-linear Regularization,ShakeDrop regularization
rkGabzZgl,H1bM1fZCW,naive,3819abfb-90af-4175-8831-a580a1bfd5eb,60,Dropout with Expectation-linear Regularization,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks
SyWvgP5el,HktJec1RZ,naive,6f51c2e8-5fe9-46d6-a1a0-b0a9bba690f0,60,EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Towards Neural Phrase-based Machine Translation
SyWvgP5el,HkpRBFxRb,KNN,043fa034-3d16-442e-a6b2-65f9bf6644a5,101,EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning
SyWvgP5el,SyZI0GWCZ,SPSM,81faecbe-e0c7-4e37-a2e3-c8f263d9d3b1,160,EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models
HkzuKpLgg,rJ4uaX2aW,KNN,61620e0a-957e-44f5-8449-66113f760bb8,80,Efficient Communications in Training Large Scale Neural Networks,Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling
HkzuKpLgg,rydeCEhs-,naive,d166679f-dd5e-4ff7-9f8c-4b844ee50458,60,Efficient Communications in Training Large Scale Neural Networks,SMASH: One-Shot Model Architecture Search through HyperNetworks
HkzuKpLgg,rypT3fb0b,SPSM,e145e928-ad9b-4c13-9593-25e43ae395dd,160,Efficient Communications in Training Large Scale Neural Networks,LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING
rJxdQ3jeg,SJaP_-xAb,SPSM,9894580e-a887-4d91-af28-54e7f4d74cda,160,End-to-end Optimized Image Compression,Deep Learning with Logged Bandit Feedback
rJxdQ3jeg,S1fHmlbCW,KNN,58b9c106-de90-41de-9b16-728f2e98e0ac,71,End-to-end Optimized Image Compression,Neural Networks for irregularly observed continuous-time Stochastic Processes
rJxdQ3jeg,H1Y8hhg0b,naive,59d18750-fd85-4626-b361-8aa99954e796,60,End-to-end Optimized Image Compression,Learning Sparse Neural Networks through L_0 Regularization
r1LXit5ee,HkCnm-bAb,KNN,c2f80194-5548-44d9-98b6-0a8cf2b03bd4,85,Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?
r1LXit5ee,By5SY2gA-,naive,c64250b7-8b59-4b8d-9a79-52f3b7703c5a,60,Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Towards Building Affect sensitive Word Distributions
r1LXit5ee,HkeJVllRW,SPSM,274c7816-de98-49ad-bac8-8451c110c616,160,Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement,Sparse-Complementary Convolution for Efficient Model Utilization on CNNs
ByToKu9ll,BydLzGb0Z,naive,c20837f2-bd2f-4cbe-8f07-35067f8ddb56,60,Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,Twin Networks: Matching the Future for Sequence Generation
ByToKu9ll,Sy4c-3xRW,SPSM,bfcd8ff9-2cc4-4fda-aac2-4c2d0d25c5ab,160,Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,DropMax: Adaptive Stochastic Softmax
ByToKu9ll,rJzIBfZAb,KNN,749e52bc-197a-4f08-ba03-a1f446597c87,158,Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models,Towards Deep Learning Models Resistant to Adversarial Attacks
SkCILwqex,Sy5OAyZC-,naive,f58f3323-008b-481d-b1b3-f9223549a5d4,60,Exploring LOTS in Deep Neural Networks,On the Use of Word Embeddings Alone to Represent Natural Language Sequences
SkCILwqex,HknbyQbC-,KNN,e6c91cad-be81-470f-b7df-93eb25881abc,107,Exploring LOTS in Deep Neural Networks,Generating Adversarial Examples with Adversarial Networks
SkCILwqex,BJjBnN9a-,SPSM,fc2b8231-2b33-41f3-8641-9eaf30828072,160,Exploring LOTS in Deep Neural Networks,Continuous Convolutional Neural Networks for Image Classification
Byx5BTilg,rJBiunlAW,naive,018b04a8-326c-40aa-b1df-dce589a83b26,60,Exploring the Application of Deep Learning for Supervised Learning Problems,Training RNNs as Fast as CNNs
Byx5BTilg,ByeqORgAW,SPSM,c3741972-acc7-4625-9962-48889147bb5a,160,Exploring the Application of Deep Learning for Supervised Learning Problems,Proximal Backpropagation
Byx5BTilg,rkTBjG-AZ,KNN,3457c73e-7524-4516-a79d-74aaeb179013,143,Exploring the Application of Deep Learning for Supervised Learning Problems,DeepArchitect: Automatically Designing and Training Deep Architectures
r1osyr_xg,HyHmGyZCZ,KNN,20a6ee5a-aef2-4ffa-abeb-a9980ef70eb5,124,Fuzzy paraphrases in learning word representations with a lexicon,Comparison of Paragram and GloVe Results for Similarity Benchmarks
r1osyr_xg,H1cWzoxA-,naive,2f012df4-7196-41c0-a345-979874986843,60,Fuzzy paraphrases in learning word representations with a lexicon,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
r1osyr_xg,rJWechg0Z,SPSM,2fe18e81-d817-4833-8381-06922b6f5417,160,Fuzzy paraphrases in learning word representations with a lexicon,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation
ry7O1ssex,rkmu5b0a-,KNN,bd472ba2-9c94-441e-834b-a64398cee4d1,138,Generative Adversarial Networks as Variational Training of Energy Based Models,MGAN: Training Generative Adversarial Nets with Multiple Generators
ry7O1ssex,Hk6kPgZA-,SPSM,fdbbe07c-1930-4d2a-8d39-fe65e68a7822,160,Generative Adversarial Networks as Variational Training of Energy Based Models,Certifying Some Distributional Robustness with Principled Adversarial Training
ry7O1ssex,ryzm6BATZ,naive,62005544-f57a-4ddb-9b06-b393af2b9f89,60,Generative Adversarial Networks as Variational Training of Energy Based Models,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks
HJWHIKqgl,Hkc-TeZ0W,SPSM,d664c1d8-5dfa-4640-bb6b-eb60f68f1b7e,160,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,A Hierarchical Model for Device Placement
HJWHIKqgl,Hk8XMWgRb,naive,08cf3977-40ee-4ade-afbc-7a617906a368,60,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,Not-So-Random Features
HJWHIKqgl,SJQHjzZ0-,KNN,b81addb6-e211-481b-98b2-3f79614edd10,97,Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,Quantitatively Evaluating GANs With Divergences Proposed for Training
rJzaDdYxx,BkrsAzWAb,SPSM,54182d7c-41b3-498d-96e0-312532e8691f,160,Gradients of Counterfactuals,Online Learning Rate Adaptation with Hypergradient Descent
rJzaDdYxx,HyzbhfWRW,KNN,94866de5-3ed9-4f68-af6b-5068006c94a2,148,Gradients of Counterfactuals,Learn to Pay Attention
rJzaDdYxx,H1meywxRW,naive,37706265-fa71-400c-98fa-0120cf561514,60,Gradients of Counterfactuals,DCN+: Mixed Objective And Deep Residual Coattention for Question Answering
r1rhWnZkg,SJ3dBGZ0Z,naive,3cc17d20-7cb4-4db8-bf6f-29cc17815158,60,Hadamard Product for Low-rank Bilinear Pooling,LSH Softmax: Sub-Linear Learning and Inference of the Softmax Layer in Deep Architectures
r1rhWnZkg,S16FPMgRZ,KNN,76e3b709-82fe-44a8-9f56-90ae501d81fd,152,Hadamard Product for Low-rank Bilinear Pooling,Tensor Contraction & Regression Networks
r1rhWnZkg,S19dR9x0b,SPSM,f814d442-70bc-4a90-908f-b1da80dd0ee0,160,Hadamard Product for Low-rank Bilinear Pooling,Alternating Multi-bit Quantization for Recurrent Neural Networks
B1KBHtcel,rJ7RBNe0-,SPSM,4e7afdb2-a560-4002-a510-09d79f678bd8,160,Here's My Point: Argumentation Mining with Pointer Networks,Generative Models for Alignment and Data Efficiency in Language
B1KBHtcel,Byt3oJ-0W,naive,257162dd-38a2-4654-8c9b-6a3477e66564,60,Here's My Point: Argumentation Mining with Pointer Networks,Learning Latent Permutations with Gumbel-Sinkhorn Networks
B1KBHtcel,rJXMpikCZ,KNN,bf040def-4764-4fdb-8f0d-38f30ef35b6b,132,Here's My Point: Argumentation Mining with Pointer Networks,Graph Attention Networks
HJeqWztlg,ryBnUWb0b,naive,468768ee-df6a-4b58-97ba-46db4528033f,60,Hierarchical compositional feature learning,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data
HJeqWztlg,SyYYPdg0-,KNN,313a2787-de29-4733-a0ad-69d252bbc9af,64,Hierarchical compositional feature learning,Counterfactual Image Networks
HJeqWztlg,Hk5elxbRW,SPSM,54c6655e-5ddc-4910-a93d-d5b023b74f95,160,Hierarchical compositional feature learning,Smooth Loss Functions for Deep Top-k Classification
Bk67W4Yxl,r1Zi2Mb0-,naive,5a67aaa9-e6c6-4fa6-8c1e-f919df3369a9,60,Improved Architectures for Computer Go,EXPLORING NEURAL ARCHITECTURE SEARCH FOR LANGUAGE TASKS
Bk67W4Yxl,HkCnm-bAb,KNN,3336bf42-270b-4e96-b2c8-06d8b4e94389,86,Improved Architectures for Computer Go,Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?
Bk67W4Yxl,ry_WPG-A-,SPSM,bd1e6d92-e8ac-436f-8edd-f8d7a115d828,160,Improved Architectures for Computer Go,On the Information Bottleneck Theory of Deep Learning
Syfkm6cgx,HkCsm6lRb,SPSM,8c5226a5-1560-41af-b54d-f54aac3acc3c,160,Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Generative Models of Visually Grounded Imagination
Syfkm6cgx,HktK4BeCZ,naive,dfd7df4a-2636-4b1f-a9c3-b8ddff484158,60,Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Learning Deep Mean Field Games for Modeling Large Population Behavior
Syfkm6cgx,rkmoiMbCb,KNN,f17a1843-e205-4322-9069-8f9a223634c8,72,Improving Invariance and Equivariance Properties of Convolutional Neural Networks,Tandem Blocks in Deep Convolutional Neural Networks
ryXZmzNeg,HJ5AUm-CZ,KNN,a52a7fc5-e2fb-4976-a8cb-00080e11da85,99,Improving Sampling from Generative Autoencoders with Markov Chains,The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples
ryXZmzNeg,SyuWNMZ0W,SPSM,6cba4743-24b5-49d7-beb0-cd841beb4f67,160,Improving Sampling from Generative Autoencoders with Markov Chains,Directing Generative Networks with Weighted Maximum Mean Discrepancy
ryXZmzNeg,HkGJUXb0-,naive,c336a7c9-e3e4-49b9-a5fa-c5720e24186d,60,Improving Sampling from Generative Autoencoders with Markov Chains,Learning Efficient Tensor Representations with Ring Structure Networks
BkVsEMYel,HyUNwulC-,SPSM,2dacdaa4-109d-420b-ba41-89b0abaffa04,160,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Parallelizing Linear Recurrent Neural Nets Over Sequence Length
BkVsEMYel,S1XolQbRW,naive,8be07123-bd08-4935-a23f-b3d6d763b725,60,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Model compression via distillation and quantization
BkVsEMYel,rkmoiMbCb,KNN,26e8dad6-9f2b-4887-8e62-1dba49c4f628,115,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,Tandem Blocks in Deep Convolutional Neural Networks
H13F3Pqll,HyFaiGbCW,naive,e16f5680-f08b-43c2-a31e-9a24b664cfc6,60,Inverse Problems in Computer Vision using  Adversarial  Imagination Priors,Generalization of Learning using Reservoir Computing
H13F3Pqll,SyYYPdg0-,SPSM,6866ef74-4770-4f79-a25d-d9fc0bd7ec06,160,Inverse Problems in Computer Vision using  Adversarial  Imagination Priors,Counterfactual Image Networks
H13F3Pqll,SyYYPdg0-,KNN,520026e0-997f-4ae9-a98d-dc300f0769a7,113,Inverse Problems in Computer Vision using  Adversarial  Imagination Priors,Counterfactual Image Networks
HJrDIpiee,rkHVZWZAZ,KNN,13e711bf-3af4-49e5-a3a7-249e3334073a,90,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning
HJrDIpiee,BJ0hF1Z0b,SPSM,509682b8-38b4-4b27-ba01-4accbb0e33c1,160,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,Learning Differentially Private Recurrent Language Models
HJrDIpiee,SkA-IE06W,naive,3d0701f1-eaba-444f-a40e-284aaff69101,60,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,When is a Convolutional Filter Easy to Learn?
Hk8rlUqge,HkbmWqxCZ,KNN,f64e7f88-754c-48aa-a9da-7c26b53e968e,66,Joint Multimodal Learning with Deep Generative Models,The Mutual Autoencoder: Controlling Information in Latent Code Representations
Hk8rlUqge,BJB7fkWR-,naive,1cb37bc2-b7ae-471a-a648-f96db0836848,60,Joint Multimodal Learning with Deep Generative Models,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games
Hk8rlUqge,H1xJjlbAZ,SPSM,19d83c57-1218-4324-b4a6-d2b203276ca7,160,Joint Multimodal Learning with Deep Generative Models,INTERPRETATION OF NEURAL NETWORK IS FRAGILE
B1vRTeqxg,Hksj2WWAW,KNN,bf4de371-332e-4b9b-8291-f0cc90d0926e,123,Learning Continuous Semantic Representations of Symbolic Expressions,Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs
B1vRTeqxg,S1fHmlbCW,SPSM,df75bf96-5275-4523-bc13-74b7c28748ac,160,Learning Continuous Semantic Representations of Symbolic Expressions,Neural Networks for irregularly observed continuous-time Stochastic Processes
B1vRTeqxg,Sy-dQG-Rb,naive,83d17591-0bc8-48e5-b3c2-06583e13cf3f,60,Learning Continuous Semantic Representations of Symbolic Expressions,Neural Speed Reading via Skim-RNN
HJ0NvFzxl,S1TgE7WR-,KNN,8fa018f0-73bf-4cd5-a97b-a4b44206492a,96,Learning Graphical State Transitions,Covariant Compositional Networks For Learning Graphs
HJ0NvFzxl,HkwrqtlR-,naive,4b40c91a-de6a-44af-9279-85dc4a17168f,60,Learning Graphical State Transitions,WHAT ARE GANS USEFUL FOR?
HJ0NvFzxl,HkwrqtlR-,SPSM,2ff1a565-71db-404d-84d0-8d96d645c429,160,Learning Graphical State Transitions,WHAT ARE GANS USEFUL FOR?
Hyq4yhile,r1lfpfZAb,naive,b58841b0-b227-4859-b934-b0f27a25acc4,60,Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,Learning to Write by Learning the Objective
Hyq4yhile,SyX0IeWAW,KNN,76fcd946-659f-4ee1-b79a-1907ef9dedb5,137,Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,META LEARNING SHARED HIERARCHIES
Hyq4yhile,Hyp3i2xRb,SPSM,6c1ae733-5a82-4eaa-9acb-4962f1c8ebc9,160,Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning,Overcoming the vanishing gradient problem in plain recurrent networks
rkE8pVcle,BJypUGZ0Z,SPSM,44ff8af3-2c52-48aa-8fbc-0aa09f1208fd,160,Learning through Dialogue Interactions by Asking Questions,Accelerating Neural Architecture Search using Performance Prediction
rkE8pVcle,SyoDInJ0-,KNN,0829a7e8-234a-4716-80c1-8f8b35f86ecf,73,Learning through Dialogue Interactions by Asking Questions,Reinforcement Learning Algorithm Selection
rkE8pVcle,SJgf6Z-0W,naive,0c624bf8-a1ae-413e-a4a8-cda744833148,60,Learning through Dialogue Interactions by Asking Questions,Predicting Multiple Actions for Stochastic Continuous Control
Skvgqgqxe,HkAClQgA-,KNN,f0be5df4-ab4c-46c4-aaba-39d9d2f9ee71,94,Learning to Compose Words into Sentences with Reinforcement Learning,A Deep Reinforced Model for Abstractive Summarization
Skvgqgqxe,SJDJNzWAZ,SPSM,d14f0eda-d497-4476-a641-68e0ad5ec158,160,Learning to Compose Words into Sentences with Reinforcement Learning,Time-Dependent Representation for Neural Event Sequence Prediction
Skvgqgqxe,B1nxTzbRZ,naive,8e79f928-34b8-4392-8e86-801e70360363,60,Learning to Compose Words into Sentences with Reinforcement Learning,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger
HJOZBvcel,rk8wKk-R-,SPSM,e765e84f-f1e1-4426-9a00-594ccbe889ee,160,Learning to Discover Sparse Graphical Models,Convolutional Sequence Modeling Revisited
HJOZBvcel,BJ_UL-k0b,naive,3fc1fe99-1972-4d42-9505-5067084e5f05,60,Learning to Discover Sparse Graphical Models,Recasting Gradient-Based Meta-Learning as Hierarchical Bayes
HJOZBvcel,S1680_1Rb,KNN,6794e237-b6cc-4109-85cf-a7f2ef78f13d,116,Learning to Discover Sparse Graphical Models,CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS
BJAFbaolg,B1nxTzbRZ,SPSM,2976b988-d62a-4095-83f4-39d6755d646a,160,Learning to Generate Samples from Noise through Infusion Training,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger
BJAFbaolg,B1EVwkqTW,naive,5fee4c69-ef6d-4798-8a3c-d2b69e609fa4,60,Learning to Generate Samples from Noise through Infusion Training,Make SVM great again with Siamese kernel for  few-shot learning
BJAFbaolg,Sy8XvGb0-,KNN,144a9f7a-f8de-4f13-bd7f-10313ae85abc,102,Learning to Generate Samples from Noise through Infusion Training,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models
r1nTpv9eg,rJbs5gbRW,naive,50137eb3-033f-4453-bd8f-5a7823cedffe,60,Learning to Perform Physics Experiments via Deep Reinforcement Learning,On the Generalization Effects of DenseNet Model Structures 
r1nTpv9eg,HyFaiGbCW,SPSM,e58b6bb7-c56a-404b-9f64-8dd295e3ac45,160,Learning to Perform Physics Experiments via Deep Reinforcement Learning,Generalization of Learning using Reservoir Computing
r1nTpv9eg,rJwelMbR-,KNN,47f0ae4b-e1ff-4dd0-be22-5e66c4cebd4d,131,Learning to Perform Physics Experiments via Deep Reinforcement Learning,Divide-and-Conquer Reinforcement Learning
rJ0-tY5xe,r1dHXnH6-,KNN,a6a2102a-74dd-486c-b5db-060bf74f8932,130,"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Natural Language Inference over Interaction Space
rJ0-tY5xe,HJDV5YxCW,SPSM,61fd8088-314d-4c29-af02-0816b72cd8e3,160,"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Heterogeneous Bitwidth Binarization in Convolutional Neural Networks
rJ0-tY5xe,SJyVzQ-C-,naive,768c06dd-9432-40a7-af78-3eb81891493e,60,"Learning to Query, Reason, and Answer Questions On Ambiguous Texts",Fraternal Dropout
r1rz6U5lg,S1sRrN-CW,naive,fc18d311-3ebc-442f-a31f-87d775e296ed,60,Learning to superoptimize programs,Revisiting Knowledge Base Embedding as Tensor Decomposition
r1rz6U5lg,H1Xw62kRZ,KNN,af94f166-75a1-4608-9101-2317d3a90421,87,Learning to superoptimize programs,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis
r1rz6U5lg,BJ7d0fW0b,SPSM,6386b7d8-07e3-444f-b7e8-9dc19be8dec3,160,Learning to superoptimize programs,Faster Reinforcement Learning with Expert State Sequences
rkFd2P5gl,HJnQJXbC-,KNN,866b3e2a-4304-47c0-a9f3-d6a26d134b4b,127,Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks
rkFd2P5gl,ryCM8zWRb,naive,3eadc753-e914-46a2-be79-dba4cc27af04,60,Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations
rkFd2P5gl,B1ZZTfZAW,SPSM,6e935aad-88db-4a03-9efe-f36c4266e20b,160,Leveraging Asynchronicity in Gradient Descent for Scalable Deep Learning,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs
Syoiqwcxx,BJQPG5lR-,SPSM,d4526410-c1ff-4991-9803-4f812c434a57,160,Local minima in training of deep networks,Avoiding degradation in deep feed-forward networks by phasing out skip-connections
Syoiqwcxx,SJDYgPgCZ,KNN,51878050-baee-468c-b389-c077f2e8810d,125,Local minima in training of deep networks,Understanding Local Minima in Neural Networks by Loss Surface Decomposition
Syoiqwcxx,r1l4eQW0Z,naive,5091c952-c908-4159-a474-d272670b83f7,60,Local minima in training of deep networks,Kernel Implicit Variational Inference
BkbY4psgg,HkUR_y-RZ,KNN,aaa8fdc7-df68-4176-a867-b1116efad26f,136,Making Neural Programming Architectures Generalize via Recursion,SEARNN: Training RNNs with global-local losses
BkbY4psgg,HJvvRoe0W,naive,24d6ca84-390d-4908-81c8-fa3937a440ba,60,Making Neural Programming Architectures Generalize via Recursion,An image representation based convolutional network for DNA classification
BkbY4psgg,ryserbZR-,SPSM,0f9621ce-474a-4508-be10-11febd5f38e1,160,Making Neural Programming Architectures Generalize via Recursion,Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach
B1akgy9xx,H1I3M7Z0b,KNN,fc85e60d-e916-4b50-b1cf-994f20a34998,117,Making Stochastic Neural Networks from Deterministic Ones,WSNet: Learning Compact and Efficient Networks with Weight Sampling
B1akgy9xx,rkdU7tCaZ,naive,f5a0f556-67be-4100-941e-de2127ce2a1c,60,Making Stochastic Neural Networks from Deterministic Ones,Dynamic Evaluation of Neural Sequence Models
B1akgy9xx,SyOK1Sg0W,SPSM,82c53deb-2381-4945-8fcd-8d7e3e206e07,160,Making Stochastic Neural Networks from Deterministic Ones,Adaptive Quantization of Neural Networks
SkJeEtclx,S1DWPP1A-,naive,b3dbfce2-b4bc-4f15-add3-89ac644f8283,60,Memory-augmented Attention Modelling for Videos,Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration
SkJeEtclx,HyzbhfWRW,SPSM,3fc05048-727e-47ab-93cb-55f2111c3b7f,160,Memory-augmented Attention Modelling for Videos,Learn to Pay Attention
SkJeEtclx,S1Euwz-Rb,KNN,4e5ac2d2-0a74-40ff-856b-340d89e3ec9c,88,Memory-augmented Attention Modelling for Videos,Compositional Attention Networks for Machine Reasoning
HJKkY35le,BJNRFNlRW,KNN,e2724c25-1cbf-485a-88d4-5172ec512700,70,Mode Regularized Generative Adversarial Networks,TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN
HJKkY35le,rkTS8lZAb,SPSM,bbc6b70d-5a5d-4f3a-8112-03bbb929d843,160,Mode Regularized Generative Adversarial Networks,Boundary Seeking GANs
HJKkY35le,H1WgVz-AZ,naive,29fae702-4f64-4485-9b4c-1bc45409b35f,60,Mode Regularized Generative Adversarial Networks,Learning Approximate Inference Networks for Structured Prediction
HyWDCXjgx,B1lMMx1CW,KNN,261d2927-3000-4ac7-998a-20faca09b793,121,Multi-label learning with the RNNs for Fashion Search,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS
HyWDCXjgx,S1sqHMZCb,naive,5031aa2e-6d65-4feb-999a-384d7caae203,60,Multi-label learning with the RNNs for Fashion Search,NerveNet: Learning Structured Policy with Graph Neural Networks
HyWDCXjgx,rk6H0ZbRb,SPSM,8a028828-0b74-4f8c-b6ac-12e259f704cd,160,Multi-label learning with the RNNs for Fashion Search,Intriguing Properties of Adversarial Examples
BJ9fZNqle,HJ5AUm-CZ,KNN,5fd77319-7035-4c48-964d-cdc8a2c173ec,105,Multi-modal Variational Encoder-Decoders,The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples
BJ9fZNqle,B1spAqUp-,SPSM,955b875f-e127-4998-b54e-d0b8e1e72d8d,160,Multi-modal Variational Encoder-Decoders,Pixel Deconvolutional Networks
BJ9fZNqle,H1u8fMW0b,naive,8157048f-b319-464e-8719-d602718dc95d,60,Multi-modal Variational Encoder-Decoders,Toward predictive machine learning for active vision
SJgWQPcxl,H1dh6Ax0Z,naive,37fd8b6c-0758-45ec-b239-8a6a288515b8,60,Multi-view Generative Adversarial Networks,TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning
SJgWQPcxl,SyzKd1bCW,SPSM,39e84dac-6caa-4ee4-8790-1a45daced428,160,Multi-view Generative Adversarial Networks,Backpropagation through the Void: Optimizing control variates for black-box gradient estimation
SJgWQPcxl,Hy7fDog0b,KNN,3a05b329-1045-4ca1-ac0d-1aeac5c543a1,139,Multi-view Generative Adversarial Networks,AmbientGAN: Generative models from lossy measurements
B1PA8fqeg,BJvWjcgAZ,KNN,25b15442-bc44-4402-8ece-e26f30dcf03e,74,Multiagent System for Layer Free Network,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update
B1PA8fqeg,Sk7KsfW0-,SPSM,addaf55c-6783-40a7-b003-6c65ca322185,160,Multiagent System for Layer Free Network,Lifelong Learning with Dynamically Expandable Networks
B1PA8fqeg,rJr4kfWCb,naive,d06a618b-3991-4b49-a1f7-4674c4416ee4,60,Multiagent System for Layer Free Network,Lung Tumor Location and Identification with AlexNet and a Custom CNN
HkEI22jeg,H1pri9vTZ,KNN,7c631364-4446-4956-902d-4c261fff5147,84,Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Deep Function Machines: Generalized Neural Networks for Topological Layer Expression
HkEI22jeg,rJhR_pxCZ,SPSM,7280be1a-19da-47cc-97c8-24cd188902ad,160,Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees
HkEI22jeg,HkAClQgA-,naive,5ac6deba-34db-4fff-be2b-80db6328a1e6,60,Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses,A Deep Reinforced Model for Abstractive Summarization
ry3iBFqgl,Hy3MvSlRW,KNN,472d5e84-90d6-4eb9-bcec-ac085ec6f5dd,129,NEWSQA: A MACHINE COMPREHENSION DATASET,Adversarial reading networks for machine comprehension
ry3iBFqgl,Hyig0zb0Z,SPSM,1bf410e9-48cf-4f01-9324-2b5b2f9b1ba7,160,NEWSQA: A MACHINE COMPREHENSION DATASET,Gated ConvNets for Letter-Based ASR
ry3iBFqgl,H1LAqMbRW,naive,d4fced54-52f0-490e-a191-7caf16658a5e,60,NEWSQA: A MACHINE COMPREHENSION DATASET,Latent forward model for Real-time Strategy game planning with incomplete information
r1Ue8Hcxg,S1v4N2l0-,KNN,fcc0c64b-9dc7-4d66-8ddf-395424dda0cd,63,Neural Architecture Search with Reinforcement Learning,Unsupervised Representation Learning by Predicting Image Rotations
r1Ue8Hcxg,ByW5yxgA-,naive,f5057eae-8a0a-499c-9a00-2524646e55d4,60,Neural Architecture Search with Reinforcement Learning,Multiscale Hidden Markov Models For Covariance Prediction
r1Ue8Hcxg,ByED-X-0W,SPSM,246e8832-846f-435e-a38b-828b8d9744ef,160,Neural Architecture Search with Reinforcement Learning,Parametric Information Bottleneck to Optimize Stochastic Neural Networks
ByW2Avqgg,rkEtzzWAb,SPSM,f826fa40-944b-407d-84ba-39d0b8052b50,160,Neural Causal Regularization under the Independence of Mechanisms Assumption,Parametric Adversarial Divergences are Good Task Losses for Generative Modeling
ByW2Avqgg,rkRwGg-0Z,KNN,43644c48-09af-4b17-a8be-52a64458f9b1,159,Neural Causal Regularization under the Independence of Mechanisms Assumption,Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs
ByW2Avqgg,r1vuQG-CW,naive,2443bb6c-8520-40a2-8959-e1dddf294860,60,Neural Causal Regularization under the Independence of Mechanisms Assumption,HexaConv
rk5upnsxe,BJQRKzbA-,SPSM,c6e8c0bf-e5ac-4bb5-ab31-afc799a613de,160,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Hierarchical Representations for Efficient Architecture Search
rk5upnsxe,HyKZyYlRZ,KNN,002b934d-8c4e-4717-a6b1-91f89be05d53,112,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,Large Scale Multi-Domain Multi-Task Learning with MultiModel
rk5upnsxe,ry1arUgCW,naive,48ca839a-aaf7-4254-b3c4-16857f96d398,60,Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes,DORA The Explorer: Directed Outreaching Reinforcement Action-Selection
S1HcOI5le,Hksj2WWAW,naive,56ab3e57-5598-4571-a0d8-971210720452,60,OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs
S1HcOI5le,HkcTe-bR-,SPSM,ae184fa5-b7ae-42c8-af40-60b3446e609f,160,OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design
S1HcOI5le,B1EVwkqTW,KNN,453264dc-4918-4aa9-a025-faf43b603ac6,119,OMG: Orthogonal Method of Grouping With Application of K-Shot Learning,Make SVM great again with Siamese kernel for  few-shot learning
H1oyRlYgg,B1tExikAW,SPSM,8e030072-e646-45bb-a8db-3dd7a72d7fca,160,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,LatentPoison -- Adversarial Attacks On The Latent Space
H1oyRlYgg,H1A5ztj3b,KNN,1388027d-b213-4431-93f7-48e1a6ebc578,118,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates
H1oyRlYgg,r1nzLmWAb,naive,0434961f-8b92-4037-b9fe-70ebef6baca1,60,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,Video Action Segmentation with Hybrid Temporal Networks
SyZprb5xg,rkZvSe-RZ,naive,5858c9a4-3aee-442b-baa0-68e0735c0481,60,On Robust Concepts and Small Neural Nets,Ensemble Adversarial Training: Attacks and Defenses
SyZprb5xg,Syjha0gAZ,SPSM,eaaf2cfa-8d26-4916-91c2-b5913815104e,160,On Robust Concepts and Small Neural Nets,Loss Functions for Multiset Prediction
SyZprb5xg,rJ33wwxRb,KNN,3b66fc02-6bd7-4d42-b565-ecbe21d08ecd,95,On Robust Concepts and Small Neural Nets,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data
SkXIrV9le,Hy_o3x-0b,KNN,fe9df44a-33d8-4380-b66c-ab580f91c84f,83,Perception Updating Networks: On architectural constraints for interpretable video generative models,Feature Map Variational Auto-Encoders
SkXIrV9le,SyL9u-WA-,naive,875d323b-c2b7-4fcb-8ba1-9e034883b0cd,60,Perception Updating Networks: On architectural constraints for interpretable video generative models,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization
SkXIrV9le,r1drp-WCZ,SPSM,0faf4bf5-9b6b-4dca-a076-2f7c7d816802,160,Perception Updating Networks: On architectural constraints for interpretable video generative models,State Space LSTM Models with Particle MCMC Inference
SJGCiw5gl,BJjBnN9a-,KNN,037a628c-7562-4d66-a0d6-4d744c261274,140,Pruning Convolutional Neural Networks for Resource Efficient Inference,Continuous Convolutional Neural Networks for Image Classification
SJGCiw5gl,HkepKG-Rb,naive,6470f232-5127-4cde-9a45-ba5f85674181,60,Pruning Convolutional Neural Networks for Resource Efficient Inference,A Semantic Loss Function for Deep Learning with Symbolic Knowledge
SJGCiw5gl,SJx9GQb0-,SPSM,34cd688c-f72c-4324-b531-ecc08116a2bb,160,Pruning Convolutional Neural Networks for Resource Efficient Inference,Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect
rJqFGTslg,SJZsR7kCZ,KNN,e0f5ca2c-6ff6-4a20-bf54-c72e3b0dc1ac,133,Pruning Filters for Efficient ConvNets,Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation
rJqFGTslg,B14TlG-RW,naive,7bdbf513-83f9-4bb7-aa6b-c5239d1ddd47,60,Pruning Filters for Efficient ConvNets,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension
rJqFGTslg,B1J_rgWRW,SPSM,7390ab71-28f2-4fa8-a4bc-c33f67c6bd6a,160,Pruning Filters for Efficient ConvNets,Understanding Deep Neural Networks with Rectified Linear Units
B1MRcPclx,r1saNM-RW,naive,0dc44a4d-b9ff-49bc-b150-76a301786b07,60,Query-Reduction Networks for Question Answering,Small Coresets to Represent Large Training Data for Support Vector Machines
B1MRcPclx,HkJ1rgbCb,SPSM,0c847a47-b381-42d5-90c7-6f2b284fcf65,160,Query-Reduction Networks for Question Answering,Using Deep Reinforcement Learning to Generate Rationales for Molecules
B1MRcPclx,BJIgi_eCZ,KNN,93eb1eba-3876-49f4-bb7d-057ad77f31e7,92,Query-Reduction Networks for Question Answering,FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension
r1VdcHcxx,HyHmGyZCZ,naive,385abbe0-c604-4fa3-8235-b75f42fa3d80,60,Recurrent Batch Normalization,Comparison of Paragram and GloVe Results for Similarity Benchmarks
r1VdcHcxx,HyKZyYlRZ,KNN,043e394f-1112-4329-9e3a-f11eb4046dfd,75,Recurrent Batch Normalization,Large Scale Multi-Domain Multi-Task Learning with MultiModel
r1VdcHcxx,SkmiegW0b,SPSM,b3f64c0b-7e75-4eeb-a664-d4a3c9807d60,160,Recurrent Batch Normalization,Challenges in Disentangling Independent Factors of Variation
B1s6xvqlx,SyvCD-b0W,SPSM,b65bb574-b85e-45ba-98e8-25226fbadc7c,160,Recurrent Environment Simulators,Autostacker: an Automatic Evolutionary Hierarchical  Machine Learning System
B1s6xvqlx,rJVruWZRW,naive,4aa443ea-63b1-44ee-a60d-5c87bb6b615c,60,Recurrent Environment Simulators,Dense Recurrent Neural Network with Attention Gate
B1s6xvqlx,HJw8fAgA-,KNN,9fc2611a-b6a2-44d0-a10f-251e02def310,108,Recurrent Environment Simulators,Learning Dynamic State Abstractions for Model-Based Reinforcement Learning
BJC8LF9ex,r1pW0WZAW,KNN,5a0648bd-ee1f-4d0a-b2e2-a35646f9916a,61,Recurrent Neural Networks for Multivariate Time Series with Missing Values,Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies
BJC8LF9ex,ByOfBggRZ,SPSM,e31436db-8f54-4aa9-9c35-14277adb7507,160,Recurrent Neural Networks for Multivariate Time Series with Missing Values,Detecting Statistical Interactions from Neural Network Weights
BJC8LF9ex,SJOl4DlCZ,naive,9aebdc05-a9fa-4836-a5db-ac53406e7f7d,60,Recurrent Neural Networks for Multivariate Time Series with Missing Values,Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier
r1GKzP5xx,BkoXnkWAb,KNN,95a8b2dd-e34d-4cd3-8c42-1ba8fd58b7af,82,Recurrent Normalization Propagation,Shifting Mean Activation Towards Zero with Bipolar Activation Functions
r1GKzP5xx,rkMt1bWAZ,SPSM,e62e387b-a20b-45ca-b1fa-8058f9f3f28c,160,Recurrent Normalization Propagation,Bias-Variance Decomposition for Boltzmann Machines
r1GKzP5xx,S1pWFzbAW,naive,6d0d6ec5-2f37-4ede-a582-938e092e0c6e,60,Recurrent Normalization Propagation,Weightless: Lossy Weight Encoding For Deep Neural Network Compression
S13wCE9xx,SkJd_y-Cb,KNN,c454b733-9e3b-4eef-a750-87b04c113801,146,Riemannian Optimization for Skip-Gram Negative Sampling,Word2net: Deep Representations of Language
S13wCE9xx,SyfiiMZA-,SPSM,3d89e730-14df-4551-a9ca-c312705c84ec,160,Riemannian Optimization for Skip-Gram Negative Sampling,Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning
S13wCE9xx,Sk2u1g-0-,naive,19deb0fe-3b25-46ee-8f26-fb3d7a9350ac,60,Riemannian Optimization for Skip-Gram Negative Sampling,Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments
H1_QSDqxl,H1Y8hhg0b,KNN,e5dfd1b4-379d-400c-8750-2201722ec1c3,76,Rule Mining in Feature Space,Learning Sparse Neural Networks through L_0 Regularization
H1_QSDqxl,B1l8BtlCb,naive,662f5c68-3845-46b9-9114-5dd108655bfb,60,Rule Mining in Feature Space,Non-Autoregressive Neural Machine Translation
H1_QSDqxl,H1Y8hhg0b,SPSM,791188f9-4d8f-4919-a4d0-8c3bad7f1f28,160,Rule Mining in Feature Space,Learning Sparse Neural Networks through L_0 Regularization
Skq89Scxx,rJ4uaX2aW,KNN,4512ac9d-8761-445b-926d-e2af49fca13b,145,SGDR: Stochastic Gradient Descent with Warm Restarts,Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling
Skq89Scxx,S1cZsf-RW,naive,39cd2495-6734-49a3-993a-21eda41f7447,60,SGDR: Stochastic Gradient Descent with Warm Restarts,WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling
Skq89Scxx,B1hcZZ-AW,SPSM,312d9721-7150-47fb-88e8-3ce8dc43ea8a,160,SGDR: Stochastic Gradient Descent with Warm Restarts,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning
HyM25Mqel,Sk03Yi10Z,naive,22c39fea-9e0f-4b36-a77f-a10ca79e795c,60,Sample Efficient Actor-Critic with  Experience Replay,An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems.
HyM25Mqel,S1ANxQW0b,KNN,66ced6b7-fe4b-48d7-923d-891f3c808fee,78,Sample Efficient Actor-Critic with  Experience Replay,Maximum a Posteriori Policy Optimisation
HyM25Mqel,BkUHlMZ0b,SPSM,22e02c3f-71a3-4e81-832a-55ddbe54fb01,160,Sample Efficient Actor-Critic with  Experience Replay,Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach
ryEGFD9gl,B1mSWUxR-,KNN,46058e7f-5228-4720-b8b1-6eb6b2b2d0d0,79,Submodular Sum-product Networks for Scene Understanding,Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML
ryEGFD9gl,Hko85plCW,naive,ad27fa17-2b86-448b-8a57-301fcb2f0b13,60,Submodular Sum-product Networks for Scene Understanding,Monotonic Chunkwise Attention
ryEGFD9gl,HyiRazbRb,SPSM,42d0180d-dea0-4d1d-9bd4-3aad83b85139,160,Submodular Sum-product Networks for Scene Understanding,Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization
BJluGHcee,Sk7cHb-C-,SPSM,0811e200-ad1f-46ec-bf58-abbdc8b21031,160,Tensorial Mixture Models,Representing dynamically: An active process for describing sequential data
BJluGHcee,S1WRibb0Z,KNN,066987b0-67de-4527-98d9-2a84d7ff3d4f,142,Tensorial Mixture Models,Expressive power of recurrent neural networks
BJluGHcee,ryvxcPeAb,naive,19edfa7a-2457-40f5-9703-0baf8fc3cd6b,60,Tensorial Mixture Models,Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient
S1jE5L5gl,r1RQdCg0W,SPSM,de6b1066-2cd4-43f8-a0fc-8af61115e608,160,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$"
S1jE5L5gl,SyhcXjy0Z,naive,fdd6f8a5-ade9-4f99-b653-ce579042caf6,60,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION
S1jE5L5gl,SJ71VXZAZ,KNN,6fb5270b-cf46-4822-aed0-a111321cb584,98,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,Learning To Generate Reviews and Discovering Sentiment
BkV4VS9ll,BkN_r2lR-,SPSM,5a74fcbc-c754-40c0-bef6-a3f278b6939c,160,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,Identifying Analogies Across Domains
BkV4VS9ll,HkinqfbAb,KNN,733a6ef2-3629-49b2-98f6-1379399e2bdc,104,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,Automatic Parameter Tying in Neural Networks
BkV4VS9ll,HkCvZXbC-,naive,57ae8f63-22d2-4064-be56-7af60545cc84,60,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY
SkC_7v5gx,SJLy_SxC-,KNN,d615aae7-4de6-44b6-acd0-2affbdcb5ae8,60,The Power of Sparsity in Convolutional Neural Networks,Log-DenseNet: How to Sparsify a DenseNet
SkC_7v5gx,Syhr6pxCW,naive,0a7dbabc-e929-4fd0-9eb4-f4cbef938772,60,The Power of Sparsity in Convolutional Neural Networks,PixelNN: Example-based Image Synthesis
SkC_7v5gx,rydeCEhs-,SPSM,7c403011-7094-4c18-876e-26b5e7151eeb,160,The Power of Sparsity in Convolutional Neural Networks,SMASH: One-Shot Model Architecture Search through HyperNetworks
HyAbMKwxe,HJewuJWCZ,SPSM,12ae04bf-811a-47d5-99af-f850c7b26286,160,Tighter bounds lead to improved classifiers,Learning to Teach
HyAbMKwxe,r1q7n9gAb,KNN,f5ad1a86-a1c9-4d62-a445-d69ecf8387ab,69,Tighter bounds lead to improved classifiers,The Implicit Bias of Gradient Descent on Separable Data
HyAbMKwxe,ryG6xZ-RZ,naive,d4c2361d-0b55-4d97-adaf-c0019c3f8d3d,60,Tighter bounds lead to improved classifiers,DLVM: A modern compiler infrastructure for deep learning systems
rJ8uNptgl,rJUBryZ0W,naive,6ad5d9ad-75bd-4b8d-bb27-3948a94912b5,60,Towards the Limit of Network Quantization,Lifelong Learning by Adjusting Priors
rJ8uNptgl,ryQu7f-RZ,SPSM,6dbdc116-def1-49df-809d-7703005f2c46,160,Towards the Limit of Network Quantization,On the Convergence of Adam and Beyond
rJ8uNptgl,S1pWFzbAW,KNN,5abe2d2d-c8f9-400f-99f8-d0c16964bf07,156,Towards the Limit of Network Quantization,Weightless: Lossy Weight Encoding For Deep Neural Network Compression
Hk3mPK5gg,SJx9GQb0-,KNN,d70f69fe-6abe-42fd-8fa4-e1434a9dd1a0,141,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect
Hk3mPK5gg,S1TgE7WR-,SPSM,2d3414b2-2a6c-4ebf-92f6-dce45e84942b,160,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Covariant Compositional Networks For Learning Graphs
Hk3mPK5gg,rywDjg-RW,naive,3abec877-9fb9-41b3-9d3a-3ca713d27df0,60,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples
ByxpMd9lx,Hkp3uhxCW,KNN,142a592d-ef03-4ab6-a441-689b01c99349,93,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Revisiting Bayes by Backprop
ByxpMd9lx,SJPpHzW0-,SPSM,0ac29235-47be-4912-8118-26cecf33f2ea,160,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Influence-Directed Explanations for Deep Convolutional Networks
ByxpMd9lx,HyiRazbRb,naive,4cbb0b95-4ef9-462c-8b77-9a59d203ef7f,60,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,Demystifying overcomplete nonlinear auto-encoders: fast SGD convergence towards sparse representation from random initialization
SJGPL9Dex,SJ19eUg0-,KNN,8eaac9a5-5f26-4a8b-92c4-c68b2a51e39f,62,Understanding Trainable Sparse Coding with Matrix Factorization,BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS
SJGPL9Dex,rkhxwltab,naive,02bc50f9-79e9-46ea-9577-dbac5f93ec6c,60,Understanding Trainable Sparse Coding with Matrix Factorization,AANN: Absolute Artificial Neural Network
SJGPL9Dex,ByW5yxgA-,SPSM,5d2dcddb-84a5-451d-b012-8a789fbb0700,160,Understanding Trainable Sparse Coding with Matrix Factorization,Multiscale Hidden Markov Models For Covariance Prediction
Sk2Im59ex,Hkbd5xZRb,naive,4eb67ecd-8659-41ce-934e-e05ece9a2668,60,Unsupervised Cross-Domain Image Generation,Spherical CNNs
Sk2Im59ex,S1Auv-WRZ,SPSM,7b6cf229-a92b-41ed-a019-9b8c33c26d23,160,Unsupervised Cross-Domain Image Generation,Data Augmentation Generative Adversarial Networks
Sk2Im59ex,ry0WOxbRZ,KNN,86118721-b10a-4baf-9e94-53b19d8ae2b6,89,Unsupervised Cross-Domain Image Generation,IVE-GAN: Invariant Encoding Generative Adversarial Networks
H1Gq5Q9el,S1PWi_lC-,SPSM,c06c847c-fd0a-4893-a25e-1ce238592e0f,160,Unsupervised Pretraining for Sequence to Sequence Learning,Multi-task Learning on MNIST Image Datasets
H1Gq5Q9el,HkwVAXyCW,KNN,2d2849b6-3981-443d-987d-d5c8377043bb,155,Unsupervised Pretraining for Sequence to Sequence Learning,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks
H1Gq5Q9el,SyunbfbAb,naive,e648dfee-b3c9-4e10-8371-8e762a85421b,60,Unsupervised Pretraining for Sequence to Sequence Learning,FigureQA: An Annotated Figure Dataset for Visual Reasoning
BysvGP5ee,HJvvRoe0W,SPSM,6cafd1d5-263c-4853-a6b3-a968c86ec5d1,160,Variational Lossy Autoencoder,An image representation based convolutional network for DNA classification
BysvGP5ee,HkbmWqxCZ,KNN,2dfc2a64-12cd-4aee-b6d4-f2be85272612,77,Variational Lossy Autoencoder,The Mutual Autoencoder: Controlling Information in Latent Code Representations
BysvGP5ee,rkRwGg-0Z,naive,1e06f374-7d43-4f4d-9339-8c0601046579,60,Variational Lossy Autoencoder,Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs
rk9eAFcxg,rJWechg0Z,KNN,db190055-e7cb-46b3-a98c-0869f9d57c78,144,Variational Recurrent Adversarial Deep Domain Adaptation,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation
rk9eAFcxg,BkM3ibZRW,SPSM,35c7ba57-5849-40fa-a2ad-ca6ea8937796,160,Variational Recurrent Adversarial Deep Domain Adaptation,Adversarially Regularized Autoencoders
rk9eAFcxg,HJ39YKiTb,naive,59ed05e6-67a8-4336-9145-5dbf0daaa102,60,Variational Recurrent Adversarial Deep Domain Adaptation,Associative Conversation Model: Generating Visual Information from Textual Information
BkmM8Dceg,SkT5Yg-RZ,SPSM,31f4fb22-c9b5-42c2-b537-8f0df8830c4c,160,Warped Convolutions: Efficient Invariance to Spatial Transformations,Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play
BkmM8Dceg,rJ7yZ2P6-,naive,3aad01e7-2c55-42be-9988-567f38e63cf4,60,Warped Convolutions: Efficient Invariance to Spatial Transformations,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus
BkmM8Dceg,H1-oTz-Cb,KNN,4bc05670-fa63-4169-8225-f48caf798951,150,Warped Convolutions: Efficient Invariance to Spatial Transformations,Parametrizing filters of a CNN with a GAN
B1hdzd5lg,rJlMAAeC-,naive,54e444a0-52e0-4873-968e-861316f76c53,60,Words or Characters? Fine-grained Gating for Reading Comprehension,Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction
B1hdzd5lg,HJWGdbbCW,SPSM,36978ee9-0e1b-4f4e-b175-ab8e31bd9b39,160,Words or Characters? Fine-grained Gating for Reading Comprehension,Reinforcement and Imitation Learning for Diverse Visuomotor Skills
B1hdzd5lg,HJRV1ZZAW,KNN,ace29ede-50bf-40b1-b0b2-0a07d3df5d19,149,Words or Characters? Fine-grained Gating for Reading Comprehension,FAST READING COMPREHENSION WITH CONVNETS
