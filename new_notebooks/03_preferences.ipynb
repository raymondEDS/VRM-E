{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921a363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1\n",
      "        rating_sum  rating_percentage\n",
      "method                               \n",
      "KNN            212           0.699670\n",
      "SPSM            53           0.174917\n",
      "naive           38           0.125413\n",
      "\n",
      "T-Test: naive vs KNN: TtestResult(statistic=-10.752658157882099, pvalue=2.5121072302919953e-18, df=99)\n",
      "T-Test: SPSM vs KNN: TtestResult(statistic=-8.73277055548224, pvalue=6.306465360934115e-14, df=99)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Table 8\n",
      "kappa 0.5621\n",
      "fleiss 0.5622\n",
      "alpha 0.5627\n",
      "scotts 0.5622\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "from nltk import agreement\n",
    "import scipy.stats as stats\n",
    "\n",
    "#checking for evaulation results\n",
    "df_labeled = pd.read_csv('../data/evaulation_data/evaluation_set_with_labels_DONT_LOOK.csv')\n",
    "\n",
    "rating_dfs = {}\n",
    "\n",
    "for initials in \"RZ KK NK\".split():\n",
    "  rating_dfs[initials] = pd.read_csv(f'../data/evaulation_data/Title_Evaluation_Final_{initials}.csv')[['unique_id','Rating']]\n",
    "\n",
    "\n",
    "df_label_rated = df_labeled.merge(rating_dfs['RZ'], on = 'unique_id').merge(rating_dfs['NK'], on = 'unique_id').merge(rating_dfs['KK'], on = 'unique_id')\n",
    "df_label_rated['rating_sum'] = df_label_rated[['Rating_x','Rating_y','Rating']].sum(axis = 1)\n",
    "\n",
    "df_label_rated['rating_percentage'] = df_label_rated['rating_sum']/303 # 300 + 3 ties\n",
    "df_performance = df_label_rated[['method','rating_sum','rating_percentage']].groupby('method').sum()\n",
    "\n",
    "\n",
    "lst_spsm = df_label_rated[df_label_rated['method']=='SPSM'][['rating_sum']].rating_sum.tolist()\n",
    "lst_KNN = df_label_rated[df_label_rated['method']=='KNN'][['rating_sum']].rating_sum.tolist()\n",
    "lst_naive = df_label_rated[df_label_rated['method']=='naive'][['rating_sum']].rating_sum.tolist()\n",
    "\n",
    "\n",
    "rater1 = df_label_rated.Rating_x.tolist()\n",
    "rater2 = df_label_rated.Rating_y.tolist()\n",
    "rater3 = df_label_rated.Rating.tolist()\n",
    "\n",
    "taskdata=[[0,str(i),str(rater1[i])] for i in range(0,len(rater1))]+[[1,str(i),str(rater2[i])] for i in range(0,len(rater2))]+[[2,str(i),str(rater3[i])] for i in range(0,len(rater3))]\n",
    "ratingtask = agreement.AnnotationTask(data=taskdata)\n",
    "\n",
    "\n",
    "print('Table 1')\n",
    "print(df_performance)\n",
    "print()\n",
    "print('T-Test: naive vs KNN:',stats.ttest_rel(lst_naive, lst_KNN))\n",
    "print('T-Test: SPSM vs KNN:',stats.ttest_rel(lst_spsm, lst_KNN))\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "print('Table 8')\n",
    "print(f'kappa {ratingtask.kappa():.4}')\n",
    "print(f'fleiss {ratingtask.multi_kappa():.4}')\n",
    "print(f'alpha {ratingtask.alpha():.4}')\n",
    "print(f'scotts {ratingtask.pi():.4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede4d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
